   CS 440/ECE 448 Assignment 6 code{white-space: pre-wrap;} span.smallcaps{font-variant: small-caps;} span.underline{text-decoration: underline;} div.column{display: inline-block; vertical-align: top; width: 50%;} div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;} ul.task-list{list-style: none;}  .CtxtMenu\_InfoClose { top:.2em; right:.2em;} .CtxtMenu\_InfoContent { overflow:auto; text-align:left; font-size:80%; padding:.4em .6em; border:1px inset; margin:1em 0px; max-height:20em; max-width:30em; background-color:#EEEEEE; white-space:normal;} .CtxtMenu\_Info.CtxtMenu\_MousePost {outline:none;} .CtxtMenu\_Info { position:fixed; left:50%; width:auto; text-align:center; border:3px outset; padding:1em 2em; background-color:#DDDDDD; color:black; cursor:default; font-family:message-box; font-size:120%; font-style:normal; text-indent:0; text-transform:none; line-height:normal; letter-spacing:normal; word-spacing:normal; word-wrap:normal; white-space:nowrap; float:none; z-index:201; border-radius: 15px; /\* Opera 10.5 and IE9 \*/ -webkit-border-radius:15px; /\* Safari and Chrome \*/ -moz-border-radius:15px; /\* Firefox \*/ -khtml-border-radius:15px; /\* Konqueror \*/ box-shadow:0px 10px 20px #808080; /\* Opera 10.5 and IE9 \*/ -webkit-box-shadow:0px 10px 20px #808080; /\* Safari 3 & Chrome \*/ -moz-box-shadow:0px 10px 20px #808080; /\* Forefox 3.5 \*/ -khtml-box-shadow:0px 10px 20px #808080; /\* Konqueror \*/ filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /\* IE \*/} .CtxtMenu\_MenuClose { position:absolute; cursor:pointer; display:inline-block; border:2px solid #AAA; border-radius:18px; -webkit-border-radius: 18px; /\* Safari and Chrome \*/ -moz-border-radius: 18px; /\* Firefox \*/ -khtml-border-radius: 18px; /\* Konqueror \*/ font-family: "Courier New", Courier; font-size:24px; color:#F0F0F0} .CtxtMenu\_MenuClose span { display:block; background-color:#AAA; border:1.5px solid; border-radius:18px; -webkit-border-radius: 18px; /\* Safari and Chrome \*/ -moz-border-radius: 18px; /\* Firefox \*/ -khtml-border-radius: 18px; /\* Konqueror \*/ line-height:0; padding:8px 0 6px /\* may need to be browser-specific \*/} .CtxtMenu\_MenuClose:hover { color:white!important; border:2px solid #CCC!important} .CtxtMenu\_MenuClose:hover span { background-color:#CCC!important} .CtxtMenu\_MenuClose:hover:focus { outline:none} .CtxtMenu\_Menu { position:absolute; background-color:white; color:black; width:auto; padding:5px 0px; border:1px solid #CCCCCC; margin:0; cursor:default; font: menu; text-align:left; text-indent:0; text-transform:none; line-height:normal; letter-spacing:normal; word-spacing:normal; word-wrap:normal; white-space:nowrap; float:none; z-index:201; border-radius: 5px; /\* Opera 10.5 and IE9 \*/ -webkit-border-radius: 5px; /\* Safari and Chrome \*/ -moz-border-radius: 5px; /\* Firefox \*/ -khtml-border-radius: 5px; /\* Konqueror \*/ box-shadow:0px 10px 20px #808080; /\* Opera 10.5 and IE9 \*/ -webkit-box-shadow:0px 10px 20px #808080; /\* Safari 3 & Chrome \*/ -moz-box-shadow:0px 10px 20px #808080; /\* Forefox 3.5 \*/ -khtml-box-shadow:0px 10px 20px #808080; /\* Konqueror \*/} .CtxtMenu\_MenuItem { padding: 1px 2em; background:transparent;} .CtxtMenu\_MenuArrow { position:absolute; right:.5em; padding-top:.25em; color:#666666; font-family: null; font-size: .75em} .CtxtMenu\_MenuActive .CtxtMenu\_MenuArrow {color:white} .CtxtMenu\_MenuArrow.CtxtMenu\_RTL {left:.5em; right:auto} .CtxtMenu\_MenuCheck { position:absolute; left:.7em; font-family: null} .CtxtMenu\_MenuCheck.CtxtMenu\_RTL { right:.7em; left:auto } .CtxtMenu\_MenuRadioCheck { position:absolute; left: .7em;} .CtxtMenu\_MenuRadioCheck.CtxtMenu\_RTL { right: .7em; left:auto} .CtxtMenu\_MenuInputBox { padding-left: 1em; right:.5em; color:#666666; font-family: null;} .CtxtMenu\_MenuInputBox.CtxtMenu\_RTL { left: .1em;} .CtxtMenu\_MenuComboBox { left:.1em; padding-bottom:.5em;} .CtxtMenu\_MenuSlider { left: .1em;} .CtxtMenu\_SliderValue { position:absolute; right:.1em; padding-top:.25em; color:#333333; font-size: .75em} .CtxtMenu\_SliderBar { outline: none; background: #d3d3d3} .CtxtMenu\_MenuLabel { padding: 1px 2em 3px 1.33em; font-style:italic} .CtxtMenu\_MenuRule { border-top: 1px solid #DDDDDD; margin: 4px 3px;} .CtxtMenu\_MenuDisabled { color:GrayText} .CtxtMenu\_MenuActive { background-color: #606872; color: white;} .CtxtMenu\_MenuDisabled:focus { background-color: #E8E8E8} .CtxtMenu\_MenuLabel:focus { background-color: #E8E8E8} .CtxtMenu\_ContextMenu:focus { outline:none} .CtxtMenu\_ContextMenu .CtxtMenu\_MenuItem:focus { outline:none} .CtxtMenu\_SelectionMenu { position:relative; float:left; border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; } .CtxtMenu\_SelectionItem { padding-right: 1em;} .CtxtMenu\_Selection { right: 40%; width:50%; } .CtxtMenu\_SelectionBox { padding: 0em; max-height:20em; max-width: none; background-color:#FFFFFF;} .CtxtMenu\_SelectionDivider { clear: both; border-top: 2px solid #000000;} .CtxtMenu\_Menu .CtxtMenu\_MenuClose { top:-10px; left:-10px} mjx-container\[jax="CHTML"\] { line-height: 0; } mjx-container \[space="1"\] { margin-left: .111em; } mjx-container \[space="2"\] { margin-left: .167em; } mjx-container \[space="3"\] { margin-left: .222em; } mjx-container \[space="4"\] { margin-left: .278em; } mjx-container \[space="5"\] { margin-left: .333em; } mjx-container \[rspace="1"\] { margin-right: .111em; } mjx-container \[rspace="2"\] { margin-right: .167em; } mjx-container \[rspace="3"\] { margin-right: .222em; } mjx-container \[rspace="4"\] { margin-right: .278em; } mjx-container \[rspace="5"\] { margin-right: .333em; } mjx-container \[size="s"\] { font-size: 70.7%; } mjx-container \[size="ss"\] { font-size: 50%; } mjx-container \[size="Tn"\] { font-size: 60%; } mjx-container \[size="sm"\] { font-size: 85%; } mjx-container \[size="lg"\] { font-size: 120%; } mjx-container \[size="Lg"\] { font-size: 144%; } mjx-container \[size="LG"\] { font-size: 173%; } mjx-container \[size="hg"\] { font-size: 207%; } mjx-container \[size="HG"\] { font-size: 249%; } mjx-container \[width="full"\] { width: 100%; } mjx-box { display: inline-block; } mjx-block { display: block; } mjx-itable { display: inline-table; } mjx-row { display: table-row; } mjx-row > \* { display: table-cell; } mjx-mtext { display: inline-block; text-align: left; } mjx-mstyle { display: inline-block; } mjx-merror { display: inline-block; color: red; background-color: yellow; } mjx-mphantom { visibility: hidden; } \_::-webkit-full-page-media, \_:future, :root mjx-container { will-change: opacity; } mjx-assistive-mml { position: absolute !important; top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0px 0px 0px !important; border: 0px !important; display: block !important; width: auto !important; overflow: hidden !important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } mjx-assistive-mml\[display="block"\] { width: 100% !important; } mjx-math { display: inline-block; text-align: left; line-height: 0; text-indent: 0; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; border-collapse: collapse; word-wrap: normal; word-spacing: normal; white-space: nowrap; direction: ltr; padding: 1px 0; } mjx-container\[jax="CHTML"\]\[display="true"\] { display: block; text-align: center; margin: 1em 0; } mjx-container\[jax="CHTML"\]\[display="true"\]\[width="full"\] { display: flex; } mjx-container\[jax="CHTML"\]\[display="true"\] mjx-math { padding: 0; } mjx-container\[jax="CHTML"\]\[justify="left"\] { text-align: left; } mjx-container\[jax="CHTML"\]\[justify="right"\] { text-align: right; } mjx-mi { display: inline-block; text-align: left; } mjx-c { display: inline-block; } mjx-utext { display: inline-block; padding: .75em 0 .2em 0; } mjx-msub { display: inline-block; text-align: left; } mjx-mo { display: inline-block; text-align: left; } mjx-stretchy-h { display: inline-table; width: 100%; } mjx-stretchy-h > \* { display: table-cell; width: 0; } mjx-stretchy-h > \* > mjx-c { display: inline-block; transform: scalex(1.0000001); } mjx-stretchy-h > \* > mjx-c::before { display: inline-block; width: initial; } mjx-stretchy-h > mjx-ext { /\* IE \*/ overflow: hidden; /\* others \*/ overflow: clip visible; width: 100%; } mjx-stretchy-h > mjx-ext > mjx-c::before { transform: scalex(500); } mjx-stretchy-h > mjx-ext > mjx-c { width: 0; } mjx-stretchy-h > mjx-beg > mjx-c { margin-right: -.1em; } mjx-stretchy-h > mjx-end > mjx-c { margin-left: -.1em; } mjx-stretchy-v { display: inline-block; } mjx-stretchy-v > \* { display: block; } mjx-stretchy-v > mjx-beg { height: 0; } mjx-stretchy-v > mjx-end > mjx-c { display: block; } mjx-stretchy-v > \* > mjx-c { transform: scaley(1.0000001); transform-origin: left center; overflow: hidden; } mjx-stretchy-v > mjx-ext { display: block; height: 100%; box-sizing: border-box; border: 0px solid transparent; /\* IE \*/ overflow: hidden; /\* others \*/ overflow: visible clip; } mjx-stretchy-v > mjx-ext > mjx-c::before { width: initial; box-sizing: border-box; } mjx-stretchy-v > mjx-ext > mjx-c { transform: scaleY(500) translateY(.075em); overflow: visible; } mjx-mark { display: inline-block; height: 0px; } mjx-TeXAtom { display: inline-block; text-align: left; } mjx-mn { display: inline-block; text-align: left; } mjx-msup { display: inline-block; text-align: left; } mjx-mfrac { display: inline-block; text-align: left; } mjx-frac { display: inline-block; vertical-align: 0.17em; padding: 0 .22em; } mjx-frac\[type="d"\] { vertical-align: .04em; } mjx-frac\[delims\] { padding: 0 .1em; } mjx-frac\[atop\] { padding: 0 .12em; } mjx-frac\[atop\]\[delims\] { padding: 0; } mjx-dtable { display: inline-table; width: 100%; } mjx-dtable > \* { font-size: 2000%; } mjx-dbox { display: block; font-size: 5%; } mjx-num { display: block; text-align: center; } mjx-den { display: block; text-align: center; } mjx-mfrac\[bevelled\] > mjx-num { display: inline-block; } mjx-mfrac\[bevelled\] > mjx-den { display: inline-block; } mjx-den\[align="right"\], mjx-num\[align="right"\] { text-align: right; } mjx-den\[align="left"\], mjx-num\[align="left"\] { text-align: left; } mjx-nstrut { display: inline-block; height: .054em; width: 0; vertical-align: -.054em; } mjx-nstrut\[type="d"\] { height: .217em; vertical-align: -.217em; } mjx-dstrut { display: inline-block; height: .505em; width: 0; } mjx-dstrut\[type="d"\] { height: .726em; } mjx-line { display: block; box-sizing: border-box; min-height: 1px; height: .06em; border-top: .06em solid; margin: .06em -.1em; overflow: hidden; } mjx-line\[type="d"\] { margin: .18em -.1em; } mjx-mrow { display: inline-block; text-align: left; } mjx-mtable { display: inline-block; text-align: center; vertical-align: .25em; position: relative; box-sizing: border-box; border-spacing: 0; border-collapse: collapse; } mjx-mstyle\[size="s"\] mjx-mtable { vertical-align: .354em; } mjx-labels { position: absolute; left: 0; top: 0; } mjx-table { display: inline-block; vertical-align: -.5ex; box-sizing: border-box; } mjx-table > mjx-itable { vertical-align: middle; text-align: left; box-sizing: border-box; } mjx-labels > mjx-itable { position: absolute; top: 0; } mjx-mtable\[justify="left"\] { text-align: left; } mjx-mtable\[justify="right"\] { text-align: right; } mjx-mtable\[justify="left"\]\[side="left"\] { padding-right: 0 ! important; } mjx-mtable\[justify="left"\]\[side="right"\] { padding-left: 0 ! important; } mjx-mtable\[justify="right"\]\[side="left"\] { padding-right: 0 ! important; } mjx-mtable\[justify="right"\]\[side="right"\] { padding-left: 0 ! important; } mjx-mtable\[align\] { vertical-align: baseline; } mjx-mtable\[align="top"\] > mjx-table { vertical-align: top; } mjx-mtable\[align="bottom"\] > mjx-table { vertical-align: bottom; } mjx-mtable\[side="right"\] mjx-labels { min-width: 100%; } mjx-mtr { display: table-row; text-align: left; } mjx-mtr\[rowalign="top"\] > mjx-mtd { vertical-align: top; } mjx-mtr\[rowalign="center"\] > mjx-mtd { vertical-align: middle; } mjx-mtr\[rowalign="bottom"\] > mjx-mtd { vertical-align: bottom; } mjx-mtr\[rowalign="baseline"\] > mjx-mtd { vertical-align: baseline; } mjx-mtr\[rowalign="axis"\] > mjx-mtd { vertical-align: .25em; } mjx-mtd { display: table-cell; text-align: center; padding: .215em .4em; } mjx-mtd:first-child { padding-left: 0; } mjx-mtd:last-child { padding-right: 0; } mjx-mtable > \* > mjx-itable > \*:first-child > mjx-mtd { padding-top: 0; } mjx-mtable > \* > mjx-itable > \*:last-child > mjx-mtd { padding-bottom: 0; } mjx-tstrut { display: inline-block; height: 1em; vertical-align: -.25em; } mjx-labels\[align="left"\] > mjx-mtr > mjx-mtd { text-align: left; } mjx-labels\[align="right"\] > mjx-mtr > mjx-mtd { text-align: right; } mjx-mtd\[extra\] { padding: 0; } mjx-mtd\[rowalign="top"\] { vertical-align: top; } mjx-mtd\[rowalign="center"\] { vertical-align: middle; } mjx-mtd\[rowalign="bottom"\] { vertical-align: bottom; } mjx-mtd\[rowalign="baseline"\] { vertical-align: baseline; } mjx-mtd\[rowalign="axis"\] { vertical-align: .25em; } mjx-c::before { display: block; width: 0; } .MJX-TEX { font-family: MJXZERO, MJXTEX; } .TEX-B { font-family: MJXZERO, MJXTEX-B; } .TEX-I { font-family: MJXZERO, MJXTEX-I; } .TEX-MI { font-family: MJXZERO, MJXTEX-MI; } .TEX-BI { font-family: MJXZERO, MJXTEX-BI; } .TEX-S1 { font-family: MJXZERO, MJXTEX-S1; } .TEX-S2 { font-family: MJXZERO, MJXTEX-S2; } .TEX-S3 { font-family: MJXZERO, MJXTEX-S3; } .TEX-S4 { font-family: MJXZERO, MJXTEX-S4; } .TEX-A { font-family: MJXZERO, MJXTEX-A; } .TEX-C { font-family: MJXZERO, MJXTEX-C; } .TEX-CB { font-family: MJXZERO, MJXTEX-CB; } .TEX-FR { font-family: MJXZERO, MJXTEX-FR; } .TEX-FRB { font-family: MJXZERO, MJXTEX-FRB; } .TEX-SS { font-family: MJXZERO, MJXTEX-SS; } .TEX-SSB { font-family: MJXZERO, MJXTEX-SSB; } .TEX-SSI { font-family: MJXZERO, MJXTEX-SSI; } .TEX-SC { font-family: MJXZERO, MJXTEX-SC; } .TEX-T { font-family: MJXZERO, MJXTEX-T; } .TEX-V { font-family: MJXZERO, MJXTEX-V; } .TEX-VB { font-family: MJXZERO, MJXTEX-VB; } mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c { font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important; } @font-face /\* 0 \*/ { font-family: MJXZERO; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Zero.woff") format("woff"); } @font-face /\* 1 \*/ { font-family: MJXTEX; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Main-Regular.woff") format("woff"); } @font-face /\* 2 \*/ { font-family: MJXTEX-B; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Main-Bold.woff") format("woff"); } @font-face /\* 3 \*/ { font-family: MJXTEX-I; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Math-Italic.woff") format("woff"); } @font-face /\* 4 \*/ { font-family: MJXTEX-MI; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Main-Italic.woff") format("woff"); } @font-face /\* 5 \*/ { font-family: MJXTEX-BI; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Math-BoldItalic.woff") format("woff"); } @font-face /\* 6 \*/ { font-family: MJXTEX-S1; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Size1-Regular.woff") format("woff"); } @font-face /\* 7 \*/ { font-family: MJXTEX-S2; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Size2-Regular.woff") format("woff"); } @font-face /\* 8 \*/ { font-family: MJXTEX-S3; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Size3-Regular.woff") format("woff"); } @font-face /\* 9 \*/ { font-family: MJXTEX-S4; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Size4-Regular.woff") format("woff"); } @font-face /\* 10 \*/ { font-family: MJXTEX-A; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_AMS-Regular.woff") format("woff"); } @font-face /\* 11 \*/ { font-family: MJXTEX-C; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Calligraphic-Regular.woff") format("woff"); } @font-face /\* 12 \*/ { font-family: MJXTEX-CB; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Calligraphic-Bold.woff") format("woff"); } @font-face /\* 13 \*/ { font-family: MJXTEX-FR; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Fraktur-Regular.woff") format("woff"); } @font-face /\* 14 \*/ { font-family: MJXTEX-FRB; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Fraktur-Bold.woff") format("woff"); } @font-face /\* 15 \*/ { font-family: MJXTEX-SS; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_SansSerif-Regular.woff") format("woff"); } @font-face /\* 16 \*/ { font-family: MJXTEX-SSB; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_SansSerif-Bold.woff") format("woff"); } @font-face /\* 17 \*/ { font-family: MJXTEX-SSI; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_SansSerif-Italic.woff") format("woff"); } @font-face /\* 18 \*/ { font-family: MJXTEX-SC; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Script-Regular.woff") format("woff"); } @font-face /\* 19 \*/ { font-family: MJXTEX-T; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Typewriter-Regular.woff") format("woff"); } @font-face /\* 20 \*/ { font-family: MJXTEX-V; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Vector-Regular.woff") format("woff"); } @font-face /\* 21 \*/ { font-family: MJXTEX-VB; src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax\_Vector-Bold.woff") format("woff"); } mjx-c.mjx-c1D461.TEX-I::before { padding: 0.626em 0.361em 0.011em 0; content: "t"; } mjx-c.mjx-c1D460.TEX-I::before { padding: 0.442em 0.469em 0.01em 0; content: "s"; } mjx-c.mjx-c1D44E.TEX-I::before { padding: 0.441em 0.529em 0.01em 0; content: "a"; } mjx-c.mjx-c1D444.TEX-I::before { padding: 0.704em 0.791em 0.194em 0; content: "Q"; } mjx-c.mjx-c28::before { padding: 0.75em 0.389em 0.25em 0; content: "("; } mjx-c.mjx-c2C::before { padding: 0.121em 0.278em 0.194em 0; content: ","; } mjx-c.mjx-c29::before { padding: 0.75em 0.389em 0.25em 0; content: ")"; } mjx-c.mjx-c1D45F.TEX-I::before { padding: 0.442em 0.451em 0.011em 0; content: "r"; } mjx-c.mjx-c2B::before { padding: 0.583em 0.778em 0.082em 0; content: "+"; } mjx-c.mjx-c31::before { padding: 0.666em 0.5em 0 0; content: "1"; } mjx-c.mjx-c2032::before { padding: 0.56em 0.275em 0 0; content: "\\2032"; } mjx-c.mjx-c1D6FE.TEX-I::before { padding: 0.441em 0.543em 0.216em 0; content: "\\3B3"; } mjx-c.mjx-c1D6FC.TEX-I::before { padding: 0.442em 0.64em 0.011em 0; content: "\\3B1"; } mjx-c.mjx-c3D::before { padding: 0.583em 0.778em 0.082em 0; content: "="; } mjx-c.mjx-c1D436.TEX-I::before { padding: 0.705em 0.76em 0.022em 0; content: "C"; } mjx-c.mjx-c1D441.TEX-I::before { padding: 0.683em 0.888em 0 0; content: "N"; } mjx-c.mjx-c2217::before { padding: 0.465em 0.5em 0 0; content: "\\2217"; } mjx-c.mjx-c61::before { padding: 0.448em 0.5em 0.011em 0; content: "a"; } mjx-c.mjx-c72::before { padding: 0.442em 0.392em 0 0; content: "r"; } mjx-c.mjx-c67::before { padding: 0.453em 0.5em 0.206em 0; content: "g"; } mjx-c.mjx-c6D::before { padding: 0.442em 0.833em 0 0; content: "m"; } mjx-c.mjx-c78::before { padding: 0.431em 0.528em 0 0; content: "x"; } mjx-c.mjx-cA0::before { padding: 0 0.25em 0 0; content: "\\A0"; } mjx-c.mjx-c1D453.TEX-I::before { padding: 0.705em 0.55em 0.205em 0; content: "f"; } mjx-c.mjx-c7B.TEX-S3::before { padding: 1.45em 0.75em 0.949em 0; content: "{"; } mjx-c.mjx-c3C::before { padding: 0.54em 0.778em 0.04em 0; content: "<"; } mjx-c.mjx-c1D452.TEX-I::before { padding: 0.442em 0.466em 0.011em 0; content: "e"; } mjx-c.mjx-c1D459.TEX-I::before { padding: 0.694em 0.298em 0.011em 0; content: "l"; } mjx-c.mjx-c30::before { padding: 0.666em 0.5em 0.022em 0; content: "0"; }

CS 440/ECE 448 Assignment 6
===========================

CS440/ECE448 Fall 2022
----------------------

Assignment 6: Reinforcement Learning
------------------------------------

### Deadline: Friday, December 2th, 11:59PM

![Snake](./CS 440_ECE 448 Assignment 6_files/Snake_can_be_completed.gif)  
From [Wikipedia](https://en.wikipedia.org/wiki/Snake_(video_game_genre))  
  

Snake is a famous video game originated in the 1976 arcade game Blockade. The player uses up, down, left and right to control the snake which grows in length (when it eats the food pellet), with the snake body and walls around the environment being the primary obstacle. In this assignment, you will train an AI agent using reinforcement learning to play a simple version of the game snake. You will implement a TD version of the Q-learning algorithm.

The external libraries required for this project are `numpy` and `pygame`. To play the game yourself and get acquainted with it, you can run

    python mp6.py --human 

* * *

Viewing Snake as a Reinforcement Learning Problem
-------------------------------------------------

For us to program an AI that plays Snake by itself, we first need to understand how the game can be viewed from the lens of reinforcement learning (RL). Broadly, RL can be completely described as an **agent** acting in an **environment**.

### The Environment

![](./CS 440_ECE 448 Assignment 6_files/snake-new.jpg)  

Our **environment** is the game board, visualized in the picture above. The green rectangle is the snake agent, and the red rectangle is the food pellet. The snake head is marked with a thicker border for easier recognition. In **utils.py**, we define some constants for sizes. Specifically:

*   The size of the entire game board is `DISPLAY_HEIGHT x DISPLAY_WIDTH`.
*   The size for every side of wall (filled with blue) is `1` (You can hardcode this number).
*   The snake head, body segment and food pellet have size `1 x 1` .
*   The snake moves with a speed of 1.

In the above picture, we have set our game board to be `10 x 18`. Therefore, we can treat the valid operating space of the snake to be a `8 x 16` grid of blocks, surrounded by a wall on each side.

You are welcome to modify `DISPLAY_HEIGHT` and `DISPLAY_WIDTH` in utils.py for your own testing.

The objective of the game is to move the snake around the grid to collect food pellets. Every time the snake/agent eats a food pellet, the points increase by 1 and the snake’s body grows one segment. When a food pellet is eaten, a new food pellet is generated randomly on the board.

The game ends when the snake dies, after which the environment rests. The snake dies if the snake head touches any of the 4 walls, or if it touches its own body (which occurs if the head goes backwards). The snake will also die after taking (8 \* ( `DISPLAY_WIDTH` ) \* ( `DISPLAY_HEIGHT` ) ) steps without food.

### The Agent

With our enviornment defined, we can now move on to the **agent**. The agent operates in the environment by defining a Markov Decision Process (MDP), which contains

1.  States: the agent’s internal representation of the environment
2.  Actions: the possible actions the agent can take in the environment
3.  Rewards: the numerical representation of the outcome of each action in the environment.

### States

Each state in the MDP is a tuple `(food_dir_x, food_dir_y, adjoining_wall_x, adjoining_wall_y, adjoining_body_top, adjoining_body_bottom, adjoining_body_left, adjoining_body_right)`.

*   **`[food_dir_x, food_dir_y]`** indicates the direction of food relative to the snake head. Each takes 3 possible values:
    
    *   **`food_dir_x`**: **0** (same coords on x axis), **1** (food on snake head left), **2** (food on snake head right)
        
    *   **`food_dir_y`**: **0** (same coords on y axis), **1** (food on snake head top), **2** (food on snake head bottom)
        
*   **`[adjoining_wall_x, adjoining_wall_y]`** indicates whether there is a wall next to the snake head. Each takes 3 possible values:
    
    *   **`adjoining_wall_x`**: **0** (no adjoining wall on x axis), **1** (wall on snake head left), **2** (wall on snake head right)
        
    *   **`adjoining_wall_y`**: **0** (no adjoining wall on y axis), **1** (wall on snake head top), **2** (wall on snake head bottom)
        
    *   Note that **`[adjoining_wall_x, adjoining_wall_y] = [0, 0]`** can also occur when the snake runs out of the board boundaries.
        
*   **`[adjoining_body_top, adjoining_body_bottom, adjoining_body_left, adjoining_body_right]`** checks if a grid next to the snake head contains the snake body. Each takes 2 possible values:
    
    *   **`adjoining_body_top`**: **1** (adjoining top square has snake body), **0** (otherwise)
    *   **`adjoining_body_bottom`**: **1** (adjoining bottom square has snake body), **0** (otherwise)
    *   **`adjoining_body_left`**: **1** (adjoining left square has snake body), **0** (otherwise)
    *   **`adjoining_body_right`**: **1** (adjoining right square has snake body), **0** (otherwise)

### Actions

In each timestep, your agent will choose an action from the set `{UP, DOWN, LEFT, RIGHT}`. You should use the respective variables defined in utils.py for these quantities.

### Rewards

In each timestep, your agent will receive a reward from the environment after taking an action. The rewards are:

*   +1 when the action results in getting the food pellet
*   \-1 when the action causes the snake to die
*   \-0.1 otherwise (does not die nor get food)

* * *

Q-Learning Agent
----------------

You will create a snake agent that learns how to get as many food pellets as possible without dying, which corresponds to maximizing the reward of the agent. In order to do this, we will use the Q-learning algorithm. Your task is to implement the TD Q-learning algorithm and train it on the MDP outlined above.

![RL Loop](./CS 440_ECE 448 Assignment 6_files/RL_loop.png)

RL Loop

In Q-learning, instead of explicitly learning a representation for transition probabilities between states, we let the agent observe its environment, choose an action, and obtain some reward. In theory, after enough iterations, the agent will implicitly learn the value for being in a state and taking an action. We refer to this quantity as the **Q-value** for the state-action pair.

Explictly, our agent interacts with it’s environment in the following feedback loop:

1.  At step t, the agent is in current state st and chooses an “optimal” action at using the learned values of Q(st,a). This acton is then executed in the environment.
2.  From the result of the action on the environment, the agent obtains a reward rt.
3.  The agent then “discretizes” this new environment by generating a state st+1 based off of the new, _post-action_ environment.
4.  With st, at, rt, and st+1, the agent can update its Q-value estimate for the state-action pair: Q(st,at).
5.  The agent is now in state st+1, and the process repeats.

Often, the notations for the current state st and next state st+1 are written as s and s′, respectively. Same for the current action a and next action a′.

### The Q-Update

The Q update formula is:

![Q-learning Equation](./CS 440_ECE 448 Assignment 6_files/Q-learning-eq.svg)

Q-learning Equation

where γ is the Temporal-Difference (TD) hyperparameter discounting future rewards, and

α\=CC+N(s,a)

is the learning rate controlling how much our Q estimate should change with each update. Unpacking this equation: C is a hyperparameter, and N(s,a) is the number of times the agent has been in state s and taken action a. As you can see, the learning rate decays as we visit a state-action pair more often.

### Choosing the Optimal Action

With its current estimate of the Q-states, the agent must choose an “optimal” action to take. However, reinforcement learning is a balancing act between exploration (visiting new states to learn their Q-values) and greed (choosing the action with the highest Q-value). Thus, during training, we use an exploration policy defined below:

a∗\=argmaxa  f(Q(s,a),N(s,a)) f(Q(s,a),N(s,a))\={1N(s,a)<NeQ(s,a)else

where Ne is a hyperparameter. Intuitively, if an action hasn’t been explored enough times (when N(s,a)<Ne), the exploration policy chooses that action regardless of its Q-value. If there are no such actions, the policy chooses the action with the highest Q value. This policy forces the agent to visit each state and action at least Ne times.

**Implementation Note:** If there is a tie among actions, break it according to the priority order `RIGHT > LEFT > DOWN > UP`.

### Implementing Your Agent

When implmenting Q-learning as described above, you will need to read and update Q and N-values. For this, we have created appropriately large tables that are defined in the Agent constructor in agent.py. You should read and write from these tables, as we will be grading part of your implementation on their contents. The order of parameters in the Q and N-tables are mentioned at the end of these instructions. Alternatively, you can look in the body of `create_q_table()` in utils.py to see how they are initialized.

Update the N-table **before** the Q-table, so that the learning rate for the very first update will be a little less than 1. This is an arbitrary choice (as long as the learning rate decays with time we effectively get the same result), but it is **necessary** to get full-credit on the autograder. To make your code cleaner, we recommend doing the N and Q-updates right next to each other in the code.

When testing, your agent no longer needs to update either table. Your agent just needs to observe its environment, generate the appropriate state, and choose the optimal action **without the exploration function**.

**Implementation Note:** Don’t forget the edge case for updating your Q and N tables when t\=0. At t\=0, both s and a will be `None`. In that case, is there anything for us to update the Q and N-tables with? Only at t\=1 will s and a correspond to a state and action for which you need to update the tables.

**Implementation Note:** When the agent “dies”, any arbitrary action can be chosen as the game will be reset before the action can be taken. This does not need to be recorded in the Q and N tables. But, you will still need to update Q and N for the action you just took that caused the death.

* * *

Grading
-------

The autograder will train and test your agent given a certain set of parameters. It will initialize the environment with various values for the initial snake and food pellet positions. The first random choice by the environment happens when it chooses a position for the second food pellet, so your Q and N-tables should exactly match ours through the time when the first food pellet is eaten.

The first set of tests will check if your Q and N-tables match ours up to the first pellet being eaten. We have provided for you 3 example tests for you to test locally:

1.  \[Test 1\] `snake_head_x=5, snake_head_y=5, food_x=2, food_y=2, Ne=40, C=40, gamma=0.7`
2.  \[Test 2\] `snake_head_x=5, snake_head_y=5, food_x=2, food_y=2, Ne=20, C=60, gamma=0.5`
3.  \[Test 3\] `snake_head_x=3, snake_head_y=3, food_x=10, food_y=4, Ne=30, C=30, gamma=0.6`

For your convenience, we have provided the expected Q and N-tables of these tests in the template code’s **data** folder. The file checkpoint1.npy corresponds to Test 1, checkpoint2.npy to Test 2, and checkpoint3.npy to Test 3. All of these tests assume a `DISPLAY_HEIGHT` of 10 and a `DISPLAY_WIDTH` of 18, which are the default values.

The major tests on the autograder will train your agent for thousands of episodes/games. In order to pass them, your code should not take too long and score enough points on average. You can test this locally as well by utilizing the command-line program mp6.py.

To see the available parameters you can set for the game, run:

    python mp6.py --help

To train and test your agent, run:

    python mp6.py [parameters]

For example, to run Test 1 above, run:

    python mp6.py --snake_head_x 5 --snake_head_y 5 --food_x 2 --food_y 2 --Ne 40 --C 40 --gamma 0.7

This will train the agent, test it, and save a local copy of your Q and N-tables in **checkpoint.npy** and **checkpoint\_N.npy**, respectively.

By default, it will train your agent for 10,000 games and test it for 1000, though you can change these by modifying the `--train-episodes` and `--test-episodes` arguments appropriately. In addition, it will also display some example games of your trained agent at the end! (If you don’t want this to happen, just change the `--show-episodes` argument to 0)

If you wish to check whether your Q and N-tables are correct, you need to first set the right parameters above and run **mp6.py** first to generate your Q and N-tables (i.e., checkpoint.npy and checkpoint\_N.npy). Then you can open the provided **check.py** file and uncomment corresponding lines to load the expected Q and N-tables. Then run:

    python check.py

If you see the terminal prints `"Your Q matrix is correct: True"` and `"Your N matrix is correct: True"`, then it means your Q and N-tables are correct.

You will **not** be tested on parameter tuning to achieve a high number of points. The autograder will pass in our choice of hyperparameter values. (So do not hard code them!) If you have implemented Q-learning correctly, you should pass all the tests with full credit. However, for fun, we recommend playing around with the hyperparameters to see how well you can train your agent!

![](./CS 440_ECE 448 Assignment 6_files/trainedAgent.gif)  
Trained Agent

* * *

Provided Code
-------------

The file [template.zip](https://courses.grainger.illinois.edu/cs440/fa2022/MPs/mp6/template.zip) contains the supplied code (described below) and the debugging examples described above.

**Do not import any non-standard libraries except pygame and numpy**

**Use numpy version <= 1.21.3.**

*   **mp6.py** - This is the main file that starts the program. This file runs the snake game with your implemented agent acting in it. The code runs a number of training games, then a number of testing games, and then displays example games at the end.
    
*   **snake.py** - This file defines the snake environment and creates the GUI for the game.
    
*   **utils.py** - This file defines environment constants as defined above and contains the functions to save and load models.
    
*   **check.py** - This file is used to check whether your Q and N-tables are the same as the given Q and N-tables for Test cases 1-3 above.
    
*   **agent.py** This is the file where you will be doing all of your work. This file contains the Agent class. This is the agent you will implement to act in the snake environment.
    

You should submit the file **agent.py** on gradescope. It may be a good idea to mess with the `DISPLAY_HEIGHT` and `DISPLAY_WIDTH` variables in utils.py. But, you’re still not submitting utils.py. Note that this means that any changes you make to `DISPLAY_HEIGHT` or `DISPLAY_WIDTH` will not be reflected in your submission, so do not hard code their values in your agent!

Inside agent.py, you will find the following variables/methods of the Agent class useful:

*   **`self._train, self._test`:** These boolean flags denote whether the agent is in train or test mode. In train mode, the agent should explore (based on the exploration function) and exploit based on the Q table. In test mode, the agent should purely exploit and always take the best action. You may assume that these variables are set appropriately. You do not need to change them.
    
*   **`self.Q, self.N`:** These numpy matrices hold the Q and N-tables, respectively. They are both of shape `(NUM_FOOD_DIR_X, NUM_FOOD_DIR_Y, NUM_ADJOINING_WALL_X_STATES, NUM_ADJOINING_WALL_Y_STATES, NUM_ADJOINING_BODY_TOP_STATES, NUM_ADJOINING_BODY_BOTTOM_STATES, NUM_ADJOINING_BODY_LEFT_STATES, NUM_ADJOINING_BODY_RIGHT_STATES,NUM_ACTIONS)`
    
*   **`self.Ne, self.C, self.gamma`:** Self-explanatory hyperparameters
    
*   **`self.reset()`:** This function resets the environment from the agent’s perspective and should be run when the agent dies.
    
*   **`self.points, self.s, self.a`:** These variables should be used to store the points, state, and action, respectively, for _bookkeeping_. That is, they will be helpful in computing whether a new food pellet has been eaten, in addition to storing _previous_ state-action pairs that will be useful when doing Q-value updates.
    

*   **`act(environment, points, dead)`:** This is the main function you will implement and is called repeatedly by **mp6.py** while games are being run.
